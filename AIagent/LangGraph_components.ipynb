{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'>\n",
      "tavily_search_results_json\n"
     ]
    }
   ],
   "source": [
    "tool = TavilySearchResults(\n",
    "    max_results=2\n",
    ") # 2 results per query\n",
    "print (type(tool))\n",
    "print (tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgentState 类定义了代理的状态数据结构\n",
    "# 这是一个 TypedDict 类型的类,用于定义具有类型提示的字典\n",
    "# 包含一个 messages 字段:\n",
    "# - messages 字段是一个列表,存储 AnyMessage 类型的消息\n",
    "# - 使用 Annotated 和 operator.add 标注,表示该字段支持列表拼接操作\n",
    "# - AnyMessage 是一个联合类型,可以是各种消息类型(AIMessage、HumanMessage等)\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要一个函数来调用`Ollama`，一个函数来检查是否存在某种action，以及一个执行action的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__ (self, model, tools, system=\"\"):\n",
    "        \"\"\"\n",
    "        初始化Agent实例\n",
    "        参数:\n",
    "            model: 要使用的语言模型,已配置好可以执行工具调用\n",
    "            tools: 可供Agent使用的工具列表\n",
    "            system: 可选的系统提示,用于指导模型行为\n",
    "        功能:\n",
    "            - 创建一个StateGraph来管理Agent的状态转换\n",
    "            - 设置节点和边来定义状态转换流程:\n",
    "              * llm节点: 调用LLM模型\n",
    "              * action节点: 执行工具操作\n",
    "            - 构建一个工具字典,用名称索引工具\n",
    "            - 将工具绑定到模型\n",
    "        \"\"\"\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_ollama)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "    \n",
    "    def exists_action(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        检查当前状态中是否存在需要执行的工具调用\n",
    "        参数:\n",
    "            state (AgentState): 当前代理状态\n",
    "        返回:\n",
    "            bool: 如果最新消息中包含工具调用则返回True,否则返回False\n",
    "        功能:\n",
    "            - 检查状态中最后一条消息是否包含工具调用\n",
    "            - 决定是否需要转到action节点或结束流程\n",
    "        \"\"\"\n",
    "        result = state['messages'][-1]\n",
    "        if hasattr(result, 'tool_calls') and result.tool_calls:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def call_ollama(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        调用Ollama模型处理当前消息流\n",
    "        参数:\n",
    "            state (AgentState): 当前代理状态,包含消息历史\n",
    "        返回:\n",
    "            dict: 包含模型响应的新状态更新\n",
    "        功能:\n",
    "            - 从状态中提取消息历史\n",
    "            - 如果有system提示,则添加到消息中\n",
    "            - 调用底层语言模型获取响应\n",
    "            - 返回模型响应作为新消息\n",
    "        \"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = state['messages']\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'message': [message]}\n",
    "    \n",
    "    def take_action(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        执行模型请求的工具调用\n",
    "        参数:\n",
    "            state (AgentState): 当前代理状态\n",
    "        返回:\n",
    "            dict: 包含工具执行结果的新状态更新\n",
    "        功能:\n",
    "            - 从最新的消息中提取所有工具调用请求\n",
    "            - 对每个工具调用:\n",
    "              * 打印调用信息\n",
    "              * 使用相应的工具执行操作\n",
    "              * 将结果封装为ToolMessage\n",
    "            - 返回所有工具调用结果作为新消息列表\n",
    "        \"\"\"\n",
    "        # 找到消息列表的最后一个消息，而且一般来说此时的最后一个消息是一个tool call，由于可能出现并行工具或并行函数调用，所以这里可能是一个tool列表\n",
    "        tool_calls = state['messages'][-1].tool_calls   \n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], result=result))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information.  \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    temperature=0\n",
    ")\n",
    "abot = Agent(model, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is the weather in sf?', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")] # 之所以要用列表，因为代理期望使用的状态具有消息属性，这是一个消息列表\n",
    "result = abot.graph.invoke({'messages': messages})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
